version: '3.8'

services:

  apiserver:
    image: nginx:alpine
    container_name: webserver
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - tts-server

  tts-server:
    build:
      context: .
      dockerfile: Dockerfile

    ports:
      - "${PORT:-8004}:8004"
    volumes:
      # Mount local config file for persistence
      - ./config.yaml:/app/config.yaml
      # Mount local directories for persistent app data
      - ./voices:/app/voices
      - ./reference_audio:/app/reference_audio
      - ./outputs:/app/outputs
      - ./logs:/app/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

    # --- GPU Access ---
    # Modern method (Recommended for newer Docker/NVIDIA setups)
    # Comment out or remove the following lines for macOS
    # devices:
    #   - nvidia.com/gpu=all
    # device_cgroup_rules:
    #   - "c 195:* rmw" # Needed for some NVIDIA container toolkit versions
    #   - "c 236:* rmw" # Needed for some NVIDIA container toolkit versions

    # Legacy method (Alternative for older Docker/NVIDIA setups)
    # If the 'devices' block above doesn't work, comment it out and uncomment
    # the 'deploy' block below. Do not use both simultaneously.
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1 # Or specify specific GPUs e.g., "device=0,1"
    #           capabilities: [gpu]
    # --- End GPU Access ---

    restart: unless-stopped
    environment:
      # Enable faster Hugging Face downloads inside the container
      - HF_HUB_ENABLE_HF_TRANSFER=1
      # Make NVIDIA GPUs visible and specify capabilities for PyTorch
      # These might not be necessary if you're not using NVIDIA GPUs
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
